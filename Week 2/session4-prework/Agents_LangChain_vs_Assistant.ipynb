{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3fdvqAdpt73a"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai tiktoken cohere arxiv duckduckgo-search langchainhub -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hExXPckwxiBx",
        "outputId": "05132c25-d683-49c0-db5d-f08bcad7d4f7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwZJqiQSuv0I"
      },
      "source": [
        "## LangChain ReAct Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKDixP3SyM4o"
      },
      "source": [
        "### Base LLM Powering RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B_8HDZr9unxL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gEOGBxy3xz-N"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(\n",
        "        content=\"Hello, how are you?\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgZQpCrYyH4g",
        "outputId": "5d73c8c0-2a88-4c04-fa58-208770c3fd60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with any questions or tasks you have. How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 13, 'total_tokens': 53}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_89f117abc5', 'finish_reason': 'stop', 'logprobs': None}, id='run-12f35913-0abc-42c5-aeb2-094bddcf2400-0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au-2Ks6kyQop"
      },
      "source": [
        "### Tool Belt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mJkFIJHpyMQD"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "tools = load_tools([\"arxiv\", \"ddg-search\"], llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSV_xhhm0HI4"
      },
      "source": [
        "### LCEL Agent Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qemUUWpou2Je"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/react-json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW1JX14k0BUH",
        "outputId": "1da93a44-1e60-4f91-f9fe-77747c567951"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react-json', 'lc_hub_commit_hash': '669cf4d6988c3b8994a8189edb3891e07948e1c0abfd500823914548c53afa7c'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names', 'tools'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \"action\" field are: {tool_names}\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='{input}\\n\\n{agent_scratchpad}'))])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lizocS910L0H"
      },
      "outputs": [],
      "source": [
        "from langchain.tools.render import render_text_description\n",
        "\n",
        "prompt = prompt.partial(\n",
        "    tools=render_text_description(tools),\n",
        "    tool_names=\", \".join([t.name for t in tools]),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OxOn0uaC0PqX"
      },
      "outputs": [],
      "source": [
        "llm_with_stop = llm.bind(stop=[\"\\nObservation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3snqga5S0Sb5"
      },
      "outputs": [],
      "source": [
        "from langchain.agents.format_scratchpad import format_log_to_str\n",
        "from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser\n",
        "\n",
        "agent = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm_with_stop\n",
        "    | ReActJsonSingleInputOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LHBkdVc90YcH"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPuZ7yxP0nwe",
        "outputId": "d7ffbe79-a0ea-4e7b-9782-34bd2415fe44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Retrieval Augmented Generation (RAG) is a concept in the field of natural language processing and machine learning. It refers to a methodology where a generative model is augmented with a retrieval component to enhance its ability to generate text. The retrieval component can fetch relevant information from a large corpus of text, which the generative model can then use to produce more accurate, informative, or contextually relevant outputs. To provide a detailed and accurate explanation, I can search for scientific articles on arxiv.org that discuss Retrieval Augmented Generation.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"arxiv\",\n",
            "  \"action_input\": \"Retrieval Augmented Generation\"\n",
            "}\n",
            "```\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2022-02-13\n",
            "Title: A Survey on Retrieval-Augmented Text Generation\n",
            "Authors: Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu\n",
            "Summary: Recently, retrieval-augmented text generation attracted increasing attention\n",
            "of the computational linguistics community. Compared with conventional\n",
            "generation models, retrieval-augmented text generation has remarkable\n",
            "advantages and particularly has achieved state-of-the-art performance in many\n",
            "NLP tasks. This paper aims to conduct a survey about retrieval-augmented text\n",
            "generation. It firstly highlights the generic paradigm of retrieval-augmented\n",
            "generation, and then it reviews notable approaches according to different tasks\n",
            "including dialogue response generation, machine translation, and other\n",
            "generation tasks. Finally, it points out some important directions on top of\n",
            "recent methods to facilitate future research.\n",
            "\n",
            "Published: 2023-12-09\n",
            "Title: Context Tuning for Retrieval Augmented Generation\n",
            "Authors: Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi\n",
            "Summary: Large language models (LLMs) have the remarkable ability to solve new tasks\n",
            "with just a few examples, but they need access to the right tools. Retrieval\n",
            "Augmented Generation (RAG) addresses this problem by retrieving a list of\n",
            "relevant tools for a given task. However, RAG's tool retrieval step requires\n",
            "all the required information to be explicitly present in the query. This is a\n",
            "limitation, as semantic search, the widely adopted tool retrieval method, can\n",
            "fail when the query is incomplete or lacks context. To address this limitation,\n",
            "we propose Context Tuning for RAG, which employs a smart context retrieval\n",
            "system to fetch relevant information that improves both tool retrieval and plan\n",
            "generation. Our lightweight context retrieval model uses numerical,\n",
            "categorical, and habitual usage signals to retrieve and rank context items. Our\n",
            "empirical results demonstrate that context tuning significantly enhances\n",
            "semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\n",
            "context retrieval and tool retrieval tasks respectively, and resulting in an\n",
            "11.6% increase in LLM-based planner accuracy. Additionally, we show that our\n",
            "proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\n",
            "outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\n",
            "plan generation, even after tool retrieval, reduces hallucination.\n",
            "\n",
            "Published: 2024-02-16\n",
            "Title: Corrective Retrieval Augmented Generation\n",
            "Authors: Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling\n",
            "Summary: Large language models (LLMs) inevitably exhibit hallucinations since the\n",
            "accuracy of generated texts cannot be secured solely by the parametric\n",
            "knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\n",
            "practicable complement to LLMs, it relies heavily on the relevance of retrieved\n",
            "documents, raising concerns about how the model behaves if retrieval goes\n",
            "wrong. To this end, we propose the Corrective Retrieval Augmented Generation\n",
            "(CRAG) to improve the robustness of generation. Specifically, a lightweight\n",
            "retrieval evaluator is designed to assess the overall quality of retrieved\n",
            "documents for a query, returning a confidence degree based on which different\n",
            "knowledge retrieval actions can be triggered. Since retrieval from static and\n",
            "limited corpora can only return sub-optimal documents, large-scale web searches\n",
            "are utilized as an extension for augmenting the retrieval results. Besides, a\n",
            "decompose-then-recompose algorithm is designed for retrieved documents to\n",
            "selectively focus on key information and filter out irrelevant information in\n",
            "them. CRAG is plug-and-play and can be seamlessly coupled with various\n",
            "RAG-based approaches. Experiments on four datasets covering short- and\n",
            "long-form generation tasks show that CRAG can significantly improve the\n",
            "performance of RAG-based approaches.\u001b[0m\u001b[32;1m\u001b[1;3mThe articles retrieved from arXiv provide a good overview of what Retrieval Augmented Generation (RAG) is and its applications in natural language processing. The first article is a survey that outlines the generic paradigm of RAG and reviews notable approaches across different tasks such as dialogue response generation and machine translation. The second article discusses a method to improve RAG by using context tuning, which helps in retrieving more relevant information when the query lacks context. The third article introduces Corrective Retrieval Augmented Generation (CRAG), which aims to improve the robustness of RAG by evaluating the quality of retrieved documents and using large-scale web searches to augment the retrieval results.\n",
            "\n",
            "Final Answer: Retrieval Augmented Generation (RAG) is a methodology in natural language processing where a generative model is augmented with a retrieval component. This retrieval component fetches relevant information from a large corpus of text, which the generative model then uses to produce more accurate, informative, or contextually relevant outputs. RAG has been applied to various tasks such as dialogue response generation and machine translation, and recent advancements have focused on improving the retrieval process through context tuning and corrective measures to ensure the relevance and quality of the information retrieved.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is Retrieval Augmented Generation?',\n",
              " 'output': 'Retrieval Augmented Generation (RAG) is a methodology in natural language processing where a generative model is augmented with a retrieval component. This retrieval component fetches relevant information from a large corpus of text, which the generative model then uses to produce more accurate, informative, or contextually relevant outputs. RAG has been applied to various tasks such as dialogue response generation and machine translation, and recent advancements have focused on improving the retrieval process through context tuning and corrective measures to ensure the relevance and quality of the information retrieved.'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\" : \"What is Retrieval Augmented Generation?\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx--DyZD1Rbf",
        "outputId": "3339197f-2ddf-4e06-a81f-ab3355cb3610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: To find out the current quarterback of the Denver Broncos, I can use a DuckDuckGo search to get the most recent information.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"duckduckgo_search\",\n",
            "  \"action_input\": \"current QB of the Denver Broncos\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mNew Broncos starting quarterback is married, a father and has backed up Tom Brady, Cam Newton, Jimmy Garoppolo - and Russell Wilson in his career. Credit: AP Photo/David Zalubowski. Denver ... ENGLEWOOD, Colo. -- The quarterback question is the bottom line of the Denver Broncos' offseason. Coach Sean Payton benched Russell Wilsonwith two games remaining in the 2023 season. The five-year ... Denver Broncos quarterback Teddy Bridgewater (5) throws against the Cincinnati Bengals during the first half of an NFL football game, Sunday, Dec. 19, 2021, in Denver. State of the Broncos roster: QB is the biggest issue and question. What the Denver Broncos do at the quarterback position will drive the offseason and possible success on the field. When it comes ... The Broncos are about to find out what they have in Jarrett Stidham, the up-until-now backup Denver quarterback who will be starting in the final two games of the 2023 season. Latest U.S.\u001b[0m\u001b[32;1m\u001b[1;3mThe search results indicate that the Denver Broncos benched Russell Wilson with two games remaining in the 2023 season. Jarrett Stidham, who was the backup quarterback, started in the final two games of the 2023 season. However, it is not explicitly stated that Jarrett Stidham is the current starting quarterback, as the situation could have changed since those games. To confirm the current starting quarterback, I would need to perform another search or find a more recent source.\n",
            "\n",
            "Thought: I will perform another search to confirm the current starting quarterback for the Denver Broncos.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"duckduckgo_search\",\n",
            "  \"action_input\": \"Denver Broncos starting quarterback 2023\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mJon Heath. follow. June 21, 2023 8:00 am MT. As we wrap up our position preview series for the Denver Broncos ahead of training camp and the 2023 season, we're down to the final — and most ... How he fared in Denver: Wilson, 34, has been the most stable starter for the Broncos since Peyton Manning retired. As of Nov. 20, 2023, he has started 25 games for Denver, completing 63.7% of his ... New Broncos starting quarterback is married, a father and has backed up Tom Brady, Cam Newton, Jimmy Garoppolo - and Russell Wilson in his career. Credit: AP Photo/David Zalubowski. Denver ... March 4, 2024 at 4:20 p.m. The Russell Wilson era ended on Monday after the Broncos told the quarterback on that they plan on releasing him after two seasons in Denver. Jarrett Stidham, who took ... The Broncos are about to find out what they have in Jarrett Stidham, the up-until-now backup Denver quarterback who will be starting in the final two games of the 2023 season. Latest U.S.\u001b[0m\u001b[32;1m\u001b[1;3mThe search results indicate that Russell Wilson was the starting quarterback for the Denver Broncos but was benched with two games remaining in the 2023 season. Jarrett Stidham started in the final two games of the 2023 season. Additionally, there is a mention of the Broncos planning on releasing Russell Wilson after two seasons, which suggests that Jarrett Stidham may be the current starting quarterback. However, the information does not explicitly confirm the current starting quarterback status post those final games.\n",
            "\n",
            "Thought: Since the search results do not provide a definitive answer on the current starting quarterback after the end of the 2023 season, I will perform one more search with a more specific query to try to get the latest information on the Denver Broncos' starting quarterback.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"duckduckgo_search\",\n",
            "  \"action_input\": \"Denver Broncos starting quarterback April 2023\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mJon Heath. follow. June 21, 2023 8:00 am MT. As we wrap up our position preview series for the Denver Broncos ahead of training camp and the 2023 season, we're down to the final — and most ... How he fared in Denver: Wilson, 34, has been the most stable starter for the Broncos since Peyton Manning retired. As of Nov. 20, 2023, he has started 25 games for Denver, completing 63.7% of his ... The Broncos are about to find out what they have in Jarrett Stidham, the up-until-now backup Denver quarterback who will be starting in the final two games of the 2023 season. Latest U.S. After resetting the franchise in 2022, the Denver Broncos have made 2023 … another reset year. The franchise jettisoned Nathaniel Hackett before the end of his debut season as head coach, eventually tabbing the Super Bowl-winning Sean Payton as his replacement.The transition to Payton sets the scene for everything the Broncos have done this offseason and what they'll do in the near future. A five-step plan for rebooting Broncos quarterback Russell Wilson. Denver Broncos quarterback Russell Wilson (3) warms up before the game at Levis Stadium on Aug. 19, 2023, in Santa Clara ...\u001b[0m\u001b[32;1m\u001b[1;3mThe search results do not provide a clear and current answer to who the starting quarterback for the Denver Broncos is as of April 2023. The information suggests that Russell Wilson was the starting quarterback for most of the season but was benched towards the end, and Jarrett Stidham started the final two games of the 2023 season. There is also mention of the Broncos planning to release Russell Wilson, but no definitive information on who the starting quarterback is right now.\n",
            "\n",
            "Thought: Since the search results are not conclusive, I will assume that the most recent starting quarterback mentioned, Jarrett Stidham, is the current starting quarterback until further information is available or until the team makes an official announcement regarding their starting quarterback for the upcoming games or season.\n",
            "\n",
            "Final Answer: Based on the most recent information available, Jarrett Stidham is believed to be the current starting quarterback for the Denver Broncos, having started the final two games of the 2023 season after Russell Wilson was benched. However, this information may change, and it is recommended to check the latest news or the team's official announcements for the most up-to-date information.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Who is the current QB of the Denver Broncos?',\n",
              " 'output': \"Based on the most recent information available, Jarrett Stidham is believed to be the current starting quarterback for the Denver Broncos, having started the final two games of the 2023 season after Russell Wilson was benched. However, this information may change, and it is recommended to check the latest news or the team's official announcements for the most up-to-date information.\"}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\" : \"Who is the current QB of the Denver Broncos?\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr9GKW0uu0Hn"
      },
      "source": [
        "## OpenAI Assistant API\n",
        "\n",
        "Helper Functions for Assistant API taking from [this](https://gist.github.com/assafelovic/579822cd42d52d80db1e1c1ff82ffffd) gist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EX64v6iM2ZY-"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def duckduckgo_search(query):\n",
        "  with DDGS() as ddgs:\n",
        "    results = [r for r in ddgs.text(query, max_results=5)]\n",
        "    return \"\\n\".join(result[\"body\"] for result in results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "6HccR0kU2x3s",
        "outputId": "6191147f-eb96-4c2c-dd11-46cc5abd7898"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Lowry has been a part of the Jets organization since June 25, 2011, when he was selected in the third round, 67 th overall, after putting up 37 points in 36 games with the Swift Current Broncos of ...\\nAdam Lowry, who has been a Jet since 2011 when he was drafted 67th overall, is the new captain of the NHL team — its third since relocating to Winnipeg from Atlanta in 2011. Andrew Ladd served ...\\nLowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ...\\nWinnipeg played without a captain last season after stripping Blake Wheeler of the title in September. Lowry is entering his 10th season with the Jets, who drafted him in the third round in 2011.\\nThe Winnipeg Jets have officially made the decision on which player will wear the captain's 'C' for the 2023-24 season, and hopefully onward. That player is 30-year-old centre Adam Lowry.\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckduckgo_search(\"Who is the current captain of the Winnipeg Jets?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NfEuZCCZ29L0"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "\n",
        "def arxiv_search(query):\n",
        "  results = []\n",
        "\n",
        "  search = arxiv.Search(\n",
        "      query = query,\n",
        "      max_results = 5,\n",
        "      sort_by = arxiv.SortCriterion.Relevance\n",
        "  )\n",
        "\n",
        "  for result in arxiv.Client().results(search):\n",
        "    results.append(result)\n",
        "\n",
        "  return \"\\n\".join(result.summary for result in results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "pGwyTXP03JgM",
        "outputId": "7be53780-8444-45cd-e315-9d452d3b57d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Existing research on response generation for chatbot focuses on \\\\textbf{First\\nResponse Generation} which aims to teach the chatbot to say the first response\\n(e.g. a sentence) appropriate to the conversation context (e.g. the user's\\nquery). In this paper, we introduce a new task \\\\textbf{Second Response\\nGeneration}, termed as Improv chat, which aims to teach the chatbot to say the\\nsecond response after saying the first response with respect the conversation\\ncontext, so as to lighten the burden on the user to keep the conversation\\ngoing. Specifically, we propose a general learning based framework and develop\\na retrieval based system which can generate the second responses with the\\nusers' query and the chatbot's first response as input. We present the approach\\nto building the conversation corpus for Improv chat from public forums and\\nsocial networks, as well as the neural networks based models for response\\nmatching and ranking. We include the preliminary experiments and results in\\nthis paper. This work could be further advanced with better deep matching\\nmodels for retrieval base systems or generative models for generation based\\nsystems as well as extensive evaluations in real-life applications.\\nThe goal of this dissertation is to investigate the PANS model capabilities\\nin providing significant improvement over RANS predictions at slightly higher\\ncomputational expense and producing LES quality results at significantly lower\\ncomputational cost. The objectives of this study are: (i) investigate the model\\nfidelity at a fixed level of scale resolution (Generation1-PANS/G1-PANS) for\\nsmooth surface separation, (ii) Derive the PANS closure model in regions of\\nresolution variation (Generation2-PANS/G2-PANS), and (iii) Validate G2-PANS\\nmodel for attached and separated flows. The separated flows considered in this\\nstudy have been designated as critical benchmark flows by NASA CFD study group.\\n  The key contributions of this dissertation are summarized as follows. The\\nturbulence closure model of varying resolution, G2-PANS, is developed by\\nderiving mathematically-consistent commutation residues and using energy\\nconservation principles. The log-layer recovery and accurate computation of\\nReynolds stress anisotropy is accomplished by transitioning from steady RANS to\\nscaled resolved simulations using the G2-PANS model. Finally, several\\nsmooth-separation flows on the NASA turbulence website have been computed with\\nhigh degree of accuracy at a significantly reduced computational effort over\\nLES using the G1-PANS and G2-PANS models.\\nWeighted Castelnuovo-Mumford Regularity and Weighted Global GenerationWe\\nintroduce and study a notion of Castelnuovo-Mumford regularity suitable for\\nweighted projective spaces.\\nMachine learning models typically suffer from the domain shift problem when\\ntrained on a source dataset and evaluated on a target dataset of different\\ndistribution. To overcome this problem, domain generalisation (DG) methods aim\\nto leverage data from multiple source domains so that a trained model can\\ngeneralise to unseen domains. In this paper, we propose a novel DG approach\\nbased on \\\\emph{Deep Domain-Adversarial Image Generation} (DDAIG). Specifically,\\nDDAIG consists of three components, namely a label classifier, a domain\\nclassifier and a domain transformation network (DoTNet). The goal for DoTNet is\\nto map the source training data to unseen domains. This is achieved by having a\\nlearning objective formulated to ensure that the generated data can be\\ncorrectly classified by the label classifier while fooling the domain\\nclassifier. By augmenting the source training data with the generated unseen\\ndomain data, we can make the label classifier more robust to unknown domain\\nchanges. Extensive experiments on four DG datasets demonstrate the\\neffectiveness of our approach.\\nHere we experiment with the use of information retrieval as an augmentation\\nfor pre-trained language models. The text corpus used in information retrieval\\ncan be viewed as form of episodic memory which grows over time. By augmenting\\nGPT 2.0 with information retrieval we achieve a zero shot 15% relative\\nreduction in perplexity on Gigaword corpus without any re-training. We also\\nvalidate our IR augmentation on an event co-reference task.\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arxiv_search(\"Retrieval Augmented Generation?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BYwzl4fj4VCn"
      },
      "outputs": [],
      "source": [
        "ddg_function = {\n",
        "    \"name\" : \"duckduckgo_search\",\n",
        "    \"description\" : \"Answer non-technical questions. Do not use this tool for questions about Machine Learning.\",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tVtU3Co25CAB"
      },
      "outputs": [],
      "source": [
        "arxiv_function = {\n",
        "    \"name\" : \"arxiv_query\",\n",
        "    \"description\" : \"Answer technical questions about the Machine Learning domain.\",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Retrieval Augmented Generation'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TOJ0_o_70CMb"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Query Assistant\",\n",
        "    instructions=\"You are a personal assistant. Use the provided functions to answer questions.\",\n",
        "    tools=[\n",
        "        {\"type\" : \"function\",\n",
        "         \"function\" : arxiv_function\n",
        "        },\n",
        "        {\"type\": \"function\",\n",
        "         \"function\" : ddg_function\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4-1106-preview\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFjuoeNS6FgW",
        "outputId": "13c79541-9185-4f62-dc1d-8a13c402ce3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant ID: asst_ClheFCt0y5DG270vcddQXPp4\n"
          ]
        }
      ],
      "source": [
        "assistant_id = assistant.id\n",
        "\n",
        "print(f\"Assistant ID: {assistant_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YYpML9_05pTW"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()\n",
        "user_input = \"What is the capital of Ottawa?\"\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=user_input,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YtjsJYk65vny"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "43ZZ8vE_6O2m"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_for_run_completion(thread_id, run_id):\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n",
        "        print(f\"Current run status: {run.status}\")\n",
        "        if run.status in ['completed', 'failed', 'requires_action']:\n",
        "            return run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfgPscjY7EMQ",
        "outputId": "f1322946-f955-4ff0-86c8-efaf3e8e0636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current run status: requires_action\n"
          ]
        }
      ],
      "source": [
        "run = wait_for_run_completion(thread.id, run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "G0nFFepT6YUK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def submit_tool_outputs(thread_id, run_id, tools_to_call):\n",
        "    tool_output_array = []\n",
        "    for tool in tools_to_call:\n",
        "        output = None\n",
        "        tool_call_id = tool.id\n",
        "        function_name = tool.function.name\n",
        "        function_args = tool.function.arguments\n",
        "\n",
        "        if function_name == \"duckduckgo_search\":\n",
        "            print(\"Consulting Duck Duck Go...\")\n",
        "            output = duckduckgo_search(query=json.loads(function_args)[\"query\"])\n",
        "\n",
        "        if function_name == \"arxiv_query\":\n",
        "            print(\"Consulting Arxiv...\")\n",
        "            output = arxiv_search(query=json.loads(function_args)[\"query\"])\n",
        "\n",
        "        if output:\n",
        "            tool_output_array.append({\"tool_call_id\": tool_call_id, \"output\": output})\n",
        "\n",
        "    print(tool_output_array)\n",
        "\n",
        "    return client.beta.threads.runs.submit_tool_outputs(\n",
        "        thread_id=thread_id,\n",
        "        run_id=run_id,\n",
        "        tool_outputs=tool_output_array\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U53cuas26pAm",
        "outputId": "be300449-b63b-4370-b4fd-814f5618eed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consulting Duck Duck Go...\n",
            "[{'tool_call_id': 'call_7TmXcXzGYmXxsbrLQul8DNlg', 'output': \"Ottawa is the capital city of Canada.It is located in the southern portion of the province of Ontario, at the confluence of the Ottawa River and the Rideau River.Ottawa borders Gatineau, Quebec, and forms the core of the Ottawa-Gatineau census metropolitan area (CMA) and the National Capital Region (NCR). As of 2021, Ottawa had a city population of 1,017,449 and a metropolitan population of ...\\nOttawa, city, capital of Canada, located in southeastern Ontario. In the eastern extreme of the province, Ottawa is situated on the Ottawa River across from Gatineau, Quebec, at the confluence of the Ottawa, Gatineau, and Rideau rivers. The Ottawa River was a key factor in the city's settlement.\\nOttawa, Ontario, incorporated as a city in 1855, population 1,017,449 (2021 census), 934,243 (2016 census).The City of Ottawa is the capital of Canada and is located on the Ottawa River on Ontario's eastern boundary with Québec, about 200 km west of Montréal.\\nOttawa is the capital city of Canada. It is located in the southern portion of the province of Ontario, at the confluence of the Ottawa River and the Rideau River. Ottawa borders Gatineau, Quebec, and forms the core of the Ottawa-Gatineau census metropolitan area (CMA) and the National Capital Region (NCR). As of 2021, Ottawa had a city population of 1,017,449 and a metropolitan population ...\\nOttawa, in the province of Ontario, is the capital of Canada. This picturesque and safe city is the fourth largest city in the country, with a population of 883,391 as of the 2011 Canadian census. It's on the eastern border of Ontario, just across the Ottawa River from Gatineau, Quebec . Ottawa is cosmopolitan, with museums, galleries ...\"}]\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n"
          ]
        }
      ],
      "source": [
        "if run.status == 'requires_action':\n",
        "    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\n",
        "    run = wait_for_run_completion(thread.id, run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4r2iKN6K6wgc"
      },
      "outputs": [],
      "source": [
        "def print_messages_from_thread(thread_id):\n",
        "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
        "    for msg in messages:\n",
        "        print(f\"{msg.role}: {msg.content[0].text.value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxNyAJ0r6u1W",
        "outputId": "8f1b4bc6-5673-4963-ced3-ec31b4333e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: Ottawa itself is the capital city of Canada and does not have a capital. It is located in the southern portion of the province of Ontario, at the confluence of the Ottawa and Rideau rivers. Ottawa also borders Gatineau, Quebec, and forms the core of the Ottawa-Gatineau census metropolitan area and the National Capital Region.\n",
            "user: What is the capital of Ottawa?\n"
          ]
        }
      ],
      "source": [
        "print_messages_from_thread(thread.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WQCciGPn_8sV"
      },
      "outputs": [],
      "source": [
        "def use_assistant(query):\n",
        "  thread = client.beta.threads.create()\n",
        "\n",
        "  message = client.beta.threads.messages.create(\n",
        "      thread_id=thread.id,\n",
        "      role=\"user\",\n",
        "      content=query,\n",
        "  )\n",
        "\n",
        "  print(\"Creating Assistant \")\n",
        "\n",
        "  run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        "  )\n",
        "\n",
        "  print(\"Querying OpenAI Assistant Thread.\")\n",
        "\n",
        "  run = wait_for_run_completion(thread.id, run.id)\n",
        "\n",
        "  if run.status == 'requires_action':\n",
        "    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\n",
        "    run = wait_for_run_completion(thread.id, run.id)\n",
        "\n",
        "  print_messages_from_thread(thread.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9CYInQ0Er0B",
        "outputId": "45f5beaf-e9e4-42d9-8889-fa5423024269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: requires_action\n",
            "Consulting Arxiv...\n",
            "[{'tool_call_id': 'call_WMktx38kZjaVPgE3i6hmxRjW', 'output': \"We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\nThe LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IRQLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\nFinetuning large language models requires huge GPU memory, restricting the\\nchoice to acquire Larger models. While the quantized version of the Low-Rank\\nAdaptation technique, named QLoRA, significantly alleviates this issue, finding\\nthe efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a\\npre-defined rank and, therefore, cannot be reconfigured for its lower ranks\\nwithout requiring further fine-tuning steps. This paper proposes QDyLoRA\\n-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach\\nfor dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to\\nefficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables\\nfine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one\\nround of fine-tuning. Experimental results show that QDyLoRA is competitive to\\nQLoRA and outperforms when employing its optimal rank.\\nLarge language models (LLMs) have demonstrated exceptional performance in\\nvarious NLP applications. However, the majority of existing open-source LLMs\\nare pre-trained primarily on English data and little part of other languages.\\nThis deficiency in multilingual training data results in suboptimal performance\\nwhen applied to languages with fewer available resources. Furthermore,\\nenhancing the performance of LLMs on low-resource languages by full-parameter\\nfine-tuning with additional data requires substantial computational resources,\\nposing computational barriers for research organizations and individual\\nresearchers. Consequently, several techniques such as parameter-efficient\\ntuning and advanced embedding initialization have been proposed to address\\nthese challenges. In this work, we combine them to facilitate cross-lingual\\ntransfer on English-dominated open-source LLM. To effectively enhance the\\nmodel's proficiency in Traditional Chinese, we conduct secondary pre-training\\non Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our\\nproposed zip-tie embedding initialization. The resulting model called Bailong,\\nwhich stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie\\nembeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B\\noptimized for multi-turn dialogue scenarios. Recognizing the inadequacy of\\nbenchmark datasets in Traditional Chinese, we further introduce Bailong-bench\\nto assess the alignment of models with human preferences and the capability to\\nfollow instructions in both Traditional Chinese and English tasks. In our\\nevaluation, Bailong-instruct 7B exhibits competitive performance on\\nBailong-bench and other benchmark datasets when compared to other open-source\\nmodels of similar or even larger parameter sizes. Bailong-instruct 7B and\\nBailong-bench are publicly available with the aim of empowering the community\\nto build upon our efforts.\\nThe growing body of political texts opens up new opportunities for rich\\ninsights into political dynamics and ideologies but also increases the workload\\nfor manual analysis. Automated speaker attribution, which detects who said what\\nto whom in a speech event and is closely related to semantic role labeling, is\\nan important processing step for computational text analysis. We study the\\npotential of the large language model family Llama 2 to automate speaker\\nattribution in German parliamentary debates from 2017-2021. We fine-tune Llama\\n2 with QLoRA, an efficient training strategy, and observe our approach to\\nachieve competitive performance in the GermEval 2023 Shared Task On Speaker\\nAttribution in German News Articles and Parliamentary Debates. Our results shed\\nlight on the capabilities of large language models in automating speaker\\nattribution, revealing a promising avenue for computational analysis of\\npolitical discourse and the development of semantic role labeling systems.\"}]\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: QLoRA stands for Quantized Low Rank Adaptation. It is an efficient fine-tuning approach designed to reduce memory usage to the extent that allows fine-tuning a large language model with 65 billion parameters on a single 48GB GPU while preserving full 16-bit fine-tuning task performance.\n",
            "\n",
            "The QLoRA method involves backpropagating gradients through a frozen, 4-bit quantized pre-trained language model into Low Rank Adapters (LoRA). The approach includes several innovations to save memory without sacrificing performance:\n",
            "\n",
            "1. **4-bit NormalFloat (NF4):** A new data type that is information theoretically optimal for normally distributed weights.\n",
            "2. **Double quantization:** This technique reduces memory footprint by quantizing the quantization constants.\n",
            "3. **Paged optimizers:** These are used to manage memory spikes during the training process.\n",
            "\n",
            "The research behind QLoRA indicates that using this fine-tuning approach on a small high-quality dataset leads to state-of-the-art results, even with smaller models than those previously considered top-performing. QLoRA also allows for extensive analysis of various models and performance across different instruction datasets and model types, which would otherwise be infeasible with regular fine-tuning due to the computational resources required.\n",
            "\n",
            "Additionally, the loquaciousness of QLoRA facilitates its use with models of considerable sizes (e.g., 33B and 65B parameters) that would typically require much more powerful hardware resources.\n",
            "\n",
            "The paper showcases a successful application of QLoRA in fine-tuning language models across diverse use-cases and proposes its efficiency for deploying accurate, compact language models on devices with limited hardware capabilities. The researchers have released the models and code associated with QLoRA, including CUDA kernels for 4-bit training, to enable further advancements and applications in the machine learning community.\n",
            "user: What is QLoRA?\n"
          ]
        }
      ],
      "source": [
        "use_assistant(\"What is QLoRA?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: Ottawa itself is the capital city of Canada and does not have a capital. It is located in the southern portion of the province of Ontario, at the confluence of the Ottawa and Rideau rivers. Ottawa also borders Gatineau, Quebec, and forms the core of the Ottawa-Gatineau census metropolitan area and the National Capital Region.\n",
            "user: What is the capital of Ottawa?\n"
          ]
        }
      ],
      "source": [
        "print_messages_from_thread(thread.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytbKC3JqI1BK",
        "outputId": "9cf3f878-5713-40bd-af20-d66006d2837a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: requires_action\n",
            "Consulting Duck Duck Go...\n",
            "[{'tool_call_id': 'call_XG1KGzf7hS2OU7DXinK9E2Ud', 'output': 'A meme is an item or genre of items that is spread widely online, such as a captioned picture or video, or an idea, behavior, style, or usage that spreads from person to person within a culture. The word comes from the Greek mimeme, meaning \"imitation\". Learn more about the origin, history, and examples of memes from the Merriam-Webster dictionary.\\nA meme (/ m iː m /; MEEM) is an idea, behavior, or style that spreads by means of imitation from person to person within a culture and often carries symbolic meaning representing a particular phenomenon or theme. A meme acts as a unit for carrying cultural ideas, symbols, or practices, that can be transmitted from one mind to another through writing, speech, gestures, rituals, or other ...\\nA meme is a cultural shorthand that portrays a particular concept or idea and is spread through online social platforms. Learn the key components, history, and examples of memes, from dancing babies to the Momo Challenge, and how to understand them better.\\nA meme is a viral image or video with text that comments on culture, events, or ideas. Learn the origins, categories, and examples of memes, and how to find and make them.\\nOver the past 25 years, memes have become one of the most dominant forms of not only comedy but communication online. There\\'s nothing you can\\'t say with a meme, and they\\'re used by everyone from Gen Alpha all the way to boomers, even becoming a tool for politicians and brands in the modern day.What used to be a niche internet phenomenon has now transcended, with knowledge of memes becoming ...'}]\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: A meme is an idea, behavior, or style that spreads by imitation from person to person within a culture and often carries symbolic meaning representing a particular phenomenon or theme. It acts as a unit for carrying cultural ideas, symbols, or practices, that can be transmitted through writing, speech, gestures, rituals, or other imitable phenomena. In the context of the internet, a meme typically refers to a captioned picture or video that goes viral online, or other forms of cultural shorthand shared on social media platforms. Memes can convey a wide range of messages, from humor to commentary on events or ideas, and have become a significant aspect of online communication and culture.\n",
            "user: What is a meme?\n"
          ]
        }
      ],
      "source": [
        "use_assistant(\"What is a meme?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
